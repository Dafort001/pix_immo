# Architecture Roadmap ‚Äì pix.immo / PixCapture

**Status**: üìã **PLANNING DOCUMENT** (keine aktive Implementierung)  
**Erstellt**: 2025-01-14  
**Zweck**: Architektonische Vorbereitung f√ºr KI-Integration, Self-Editing, Avatar-/Video-Generation und externe Exporte

---

## √úbersicht

Dieses Dokument definiert die zuk√ºnftige Architektur f√ºr:
- **KI-Pipeline**: Vision (fal.ai) + GPT (OpenAI) f√ºr Bildanalyse und Captions
- **Self-Editing**: Objektentfernung in PixCapture-App
- **Avatar/Video**: AI-generierte Avatare und Video-Content
- **Externe Exporte**: ArtSpace, ScenePlates, 360¬∞-Pano

**Wichtig**: Keines dieser Features ist aktuell implementiert. Diese Planung verhindert sp√§tere Architektur-Umbauten.

---

## 1. R2 Object Storage Struktur

### Definierte Prefixes (Cloudflare R2)

```
raw/{shoot_id}/...
‚îú‚îÄ‚îÄ {image_id}_g001_e-2.dng          # RAW-Bracket (Underexposed)
‚îú‚îÄ‚îÄ {image_id}_g001_e0.dng           # RAW-Bracket (Normal)
‚îî‚îÄ‚îÄ {image_id}_g001_e+2.dng          # RAW-Bracket (Overexposed)

archive/{shoot_id}/...
‚îî‚îÄ‚îÄ {image_id}_hr.jpg                 # Highres-JPEG vom Editing-Team (final entwickelt)

master/{shoot_id}/...
‚îî‚îÄ‚îÄ {image_id}_3000.jpg               # 3000px Master-Version (Basis f√ºr KI + Galerie)

analysis/{shoot_id}/...
‚îú‚îÄ‚îÄ {image_id}.vision.json            # fal.ai Florence-2 Detection-Ergebnisse
‚îú‚îÄ‚îÄ {image_id}.mask_sky.png           # SAM2 Segmentierungsmasken
‚îú‚îÄ‚îÄ {image_id}.mask_floor.png
‚îú‚îÄ‚îÄ {image_id}.gpt.json               # OpenAI GPT-Vision Captions & Metadata
‚îî‚îÄ‚îÄ summary.json                       # Konsolidierte Shoot-Zusammenfassung (sp√§ter)

edits/{shoot_id}/{image_id}/...
‚îú‚îÄ‚îÄ base.jpg                          # Kopie von master/ (erste Self-Edit-Basis)
‚îú‚îÄ‚îÄ clean_v1.jpg                      # Objektentfernung Version 1
‚îú‚îÄ‚îÄ clean_v2.jpg                      # Objektentfernung Version 2
‚îî‚îÄ‚îÄ ...
```

### Aktueller Status
- ‚úÖ `raw/` bereits in Nutzung (Mobile Upload Workflow)
- ‚è∏Ô∏è `archive/`, `master/`, `analysis/`, `edits/` - reserviert, nicht aktiv

---

## 2. Datenbank-Felder (images-Tabelle)

### Geplante Erweiterungen

```typescript
// shared/schema.ts - images table extensions (FUTURE)
export const images = pgTable("images", {
  // ... existing fields ...

  // KI-Pipeline Status Flags
  ready_for_vision: boolean("ready_for_vision").notNull().default(false),
  vision_done: boolean("vision_done").notNull().default(false),
  gpt_done: boolean("gpt_done").notNull().default(false),
  
  // Self-Editing Flags
  has_edits: boolean("has_edits").notNull().default(false),
  best_version: varchar("best_version", { length: 50 }).default("master"), // "master", "clean_v1", ...
  
  // Multi-Tenant/Context (optional, f√ºr sp√§tere Erweiterungen)
  shoot_code: varchar("shoot_code", { length: 10 }), // e.g. "AB3KQ"
  room_type: varchar("room_type", { length: 50 }),   // e.g. "wohnzimmer", "kueche"
  customer_id: varchar("customer_id", { length: 50 }),
  photographer_id: varchar("photographer_id", { length: 50 }),
});
```

### Aktueller Status
- ‚è∏Ô∏è Felder NICHT hinzugef√ºgt (HALT aktiv)
- üìã Schema dokumentiert f√ºr zuk√ºnftige Migration

---

## 3. Editor-Return als KI-Einstiegspunkt

### Workflow (geplant)

```
1. Editing-Team liefert Highres-JPEG zur√ºck
   ‚îî‚îÄ> Speichern: archive/{shoot_id}/{image_id}_hr.jpg

2. Backend erzeugt 3000px Master-Version
   ‚îî‚îÄ> Speichern: master/{shoot_id}/{image_id}_3000.jpg
   ‚îî‚îÄ> Sharp resize: width=3000, quality=92, progressive

3. DB-Flags setzen
   ‚îî‚îÄ> ready_for_vision = true
   ‚îî‚îÄ> vision_done = false
   ‚îî‚îÄ> gpt_done = false

4. KI-Orchestrierung (sp√§ter)
   ‚îî‚îÄ> Vision-Orchestrator liest master/, schreibt analysis/
   ‚îî‚îÄ> GPT-Orchestrator liest master/ + vision.json, schreibt gpt.json
```

### Aktueller Status
- ‚è∏Ô∏è Editor-Return API NICHT implementiert
- üìã Workflow definiert als Einstiegspunkt f√ºr KI-Pipeline

---

## 4. Service-Hooks / Orchestratoren

### Geplante Backend-Services (Struktur-Platzhalter)

```
server/services/
‚îú‚îÄ‚îÄ vision-orchestrator/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts                      # fal.ai Florence-2 + SAM2 Integration
‚îÇ   ‚îú‚îÄ‚îÄ object-detection.ts           # VISION_OBJECT_DETECTION_MODEL
‚îÇ   ‚îú‚îÄ‚îÄ caption.ts                    # VISION_CAPTION_MODEL
‚îÇ   ‚îú‚îÄ‚îÄ regional-caption.ts           # VISION_REGIONAL_CAPTION_MODEL
‚îÇ   ‚îú‚îÄ‚îÄ segmentation.ts               # VISION_SEGMENTATION_MODEL (SAM2)
‚îÇ   ‚îî‚îÄ‚îÄ object-removal.ts             # VISION_OBJECT_REMOVAL_MODEL
‚îÇ
‚îú‚îÄ‚îÄ gpt-orchestrator/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts                      # OpenAI GPT-Vision Integration
‚îÇ   ‚îú‚îÄ‚îÄ caption-generator.ts          # Deutsche/englische Bildbeschreibungen
‚îÇ   ‚îú‚îÄ‚îÄ metadata-extractor.ts         # Room type, features, QC-Checks
‚îÇ   ‚îî‚îÄ‚îÄ expose-text-generator.ts      # Marketing-Texte aus Bildanalyse
‚îÇ
‚îú‚îÄ‚îÄ avatar-orchestrator/              # Platzhalter f√ºr Avatar-Generation
‚îÇ   ‚îî‚îÄ‚îÄ index.ts                      # Input: summary.json ‚Üí Video/Avatar
‚îÇ
‚îú‚îÄ‚îÄ video-orchestrator/               # Platzhalter f√ºr Video-Generation
‚îÇ   ‚îî‚îÄ‚îÄ index.ts                      # Sora/Veo/fal-Video Integration
‚îÇ
‚îú‚îÄ‚îÄ gallery-export-orchestrator/      # ArtSpace-Hook
‚îÇ   ‚îî‚îÄ‚îÄ index.ts                      # Export zu 3D-Galerie-Plattformen
‚îÇ
‚îú‚îÄ‚îÄ sceneplates-export-orchestrator/  # ScenePlates-Hook
‚îÇ   ‚îî‚îÄ‚îÄ index.ts                      # Automotive/Backplate-Exporte
‚îÇ
‚îú‚îÄ‚îÄ pano-export-orchestrator/         # 360¬∞-Hook
‚îÇ   ‚îî‚îÄ‚îÄ index.ts                      # Pano2VR/Gaussian-Splatting-Exports
‚îÇ
‚îî‚îÄ‚îÄ external-sync-orchestrator/       # Generische Reserve-Schnittstelle
    ‚îî‚îÄ‚îÄ index.ts                      # Webhooks f√ºr externe Systeme
```

### Aktueller Status
- ‚è∏Ô∏è Services NICHT erstellt (nur Struktur-Planung)
- üìã Ordner-Hierarchie definiert f√ºr sp√§tere Entwicklung

---

## 5. Environment Variables (geplant)

### Vision Provider (fal.ai)

```bash
# Vision Provider Configuration
VISION_PROVIDER=fal
VISION_API_BASE_URL=https://fal.run
VISION_API_KEY=<secret>

# fal.ai Model Endpoints
VISION_OBJECT_DETECTION_MODEL=fal-ai/florence-2-large/object-detection
VISION_CAPTION_MODEL=fal-ai/florence-2-large/caption
VISION_REGIONAL_CAPTION_MODEL=fal-ai/florence-2-large/regional-caption
VISION_SEGMENTATION_MODEL=fal-ai/sam2
VISION_OBJECT_REMOVAL_MODEL=fal-ai/object-removal

# Vision Service Limits
VISION_MAX_CONCURRENT_JOBS=5
VISION_REQUEST_TIMEOUT_MS=30000
VISION_JOB_RETRY_COUNT=3
```

### GPT (OpenAI Vision)

```bash
# OpenAI Configuration (already exists: OPENAI_API_KEY)
OPENAI_API_KEY=<secret>
GPT_VISION_MODEL=gpt-4o              # oder gpt-4.5-vision (wenn verf√ºgbar)
GPT_LOCALE_DEFAULT=de                # Deutsche Captions als Standard
GPT_MAX_TOKENS=1000
GPT_TEMPERATURE=0.7
```

### Avatar / Video (Platzhalter)

```bash
# Avatar Generation (zuk√ºnftig)
AVATAR_PROVIDER=<TBD>
AVATAR_API_KEY=<secret>
AVATAR_MODEL=<TBD>

# Video Generation (fal.ai oder andere)
VIDEO_GEN_PROVIDER=fal
VIDEO_GEN_MODEL=<TBD>
VIDEO_MOTION_FILL_MODEL=<TBD>
VISION_OUTPAINT_MODEL=<TBD>
```

### Aktueller Status
- ‚úÖ `OPENAI_API_KEY` bereits vorhanden
- ‚è∏Ô∏è Neue ENV-Variablen NICHT hinzugef√ºgt (warten auf Implementation)
- üìã Naming-Convention definiert f√ºr sp√§tere Konfiguration

---

## 6. Self-Edit / PixCapture API (geplant)

### Objektentfernung Workflow

```typescript
// FUTURE API Route (NOT IMPLEMENTED)
POST /api/remove-object

// Request Body
{
  shoot_id: string,
  image_id: string,
  version: "master" | "clean_v1" | "clean_v2" | ...,
  mask: {
    format: "png" | "pixelmap",
    data: string // Base64-encoded mask
  }
}

// Workflow
1. Bild laden aus master/{shoot_id}/ oder edits/{shoot_id}/{image_id}/
2. Maske + Bild an VISION_OBJECT_REMOVAL_MODEL (fal.ai) senden
3. Ergebnis speichern:
   - edits/{shoot_id}/{image_id}/clean_vX.jpg
4. DB-Update:
   - has_edits = true
   - best_version = "clean_vX"

// Response
{
  success: true,
  new_version: "clean_v3",
  preview_url: "https://r2.../edits/AB3KQ/img_001/clean_v3.jpg"
}
```

### PixCapture Integration

Die PixCapture-App erh√§lt sp√§ter:
- Canvas-Tool f√ºr Masken-Zeichnung (Pinsel-Interface)
- API-Call zu `/api/remove-object`
- Echtzeit-Preview der bereinigten Version
- Version-History (clean_v1, v2, v3...)

### Aktueller Status
- ‚è∏Ô∏è API-Route NICHT implementiert
- üìã Workflow dokumentiert f√ºr Self-Editing Feature

---

## 7. Summary pro Shoot (Avatar-/Video-Hook)

### summary.json Format (geplant)

```json
// analysis/{shoot_id}/summary.json
{
  "shoot_id": "AB3KQ",
  "shoot_code": "AB3KQ",
  "property": {
    "name": "Musterwohnung Eppendorf",
    "address": "Eppendorfer Weg 42, 20259 Hamburg",
    "type": "apartment",
    "size_sqm": 85
  },
  "images": {
    "total": 42,
    "by_room": {
      "wohnzimmer": 8,
      "kueche": 6,
      "schlafzimmer": 5,
      "bad": 4,
      "balkon": 3,
      "flur": 2,
      "fassade": 2
    }
  },
  "highlights": [
    "Modernes Design mit gro√üen Fenstern",
    "Hochwertige K√ºche mit Miele-Ger√§ten",
    "Balkon mit Blick ins Gr√ºne"
  ],
  "gpt_aggregated": {
    "common_features": ["modern", "hell", "gepflegt"],
    "quality_score": 8.5,
    "recommended_for": ["young_professionals", "couples"]
  },
  "vision_stats": {
    "detected_objects": {
      "furniture": 156,
      "windows": 24,
      "doors": 12
    },
    "dominant_colors": ["white", "beige", "gray"]
  },
  "export_ready": {
    "avatar": false,
    "video": false,
    "artspace": false
  },
  "generated_at": 1705219200000
}
```

### Verwendung

- **Avatar-Orchestrator**: Generiert AI-Avatar mit Sprachausgabe basierend auf summary.json
- **Video-Orchestrator**: Erzeugt Property-Tour-Video aus Bildsequenzen + GPT-Texten
- **ArtSpace-Export**: Bereitet 3D-Galerie-Daten vor
- **ScenePlates-Export**: Automotive-Backplates mit Metadaten

### Aktueller Status
- ‚è∏Ô∏è summary.json NICHT generiert
- üìã JSON-Schema dokumentiert als Input f√ºr nachgelagerte Services

---

## 8. Implementation Priorities (FUTURE)

### Phase 1: KI-Pipeline Foundation
1. Editor-Return API implementieren
2. 3000px Master-Generation (Sharp)
3. DB-Migration: Flags hinzuf√ºgen (ready_for_vision, vision_done, gpt_done)
4. R2-Prefixes: archive/, master/ aktivieren

### Phase 2: Vision Integration
1. fal.ai SDK einbinden
2. Vision-Orchestrator implementieren
3. Object-Detection + Segmentation (Florence-2, SAM2)
4. analysis/{shoot_id}/ Ergebnisse speichern

### Phase 3: GPT Integration
1. OpenAI GPT-Vision API integrieren
2. GPT-Orchestrator implementieren
3. Deutsche/englische Captions generieren
4. gpt.json nach analysis/ schreiben

### Phase 4: Self-Editing
1. POST /api/remove-object implementieren
2. PixCapture: Canvas-Tool f√ºr Masken
3. edits/{shoot_id}/{image_id}/ Versionierung
4. has_edits, best_version DB-Logik

### Phase 5: Avatar/Video/Export
1. summary.json Generator implementieren
2. Avatar-Orchestrator (TBD Provider)
3. Video-Orchestrator (Sora/Veo/fal)
4. Externe Exporte (ArtSpace, ScenePlates, 360¬∞)

---

## 9. Technische Abh√§ngigkeiten

### Neue Package-Dependencies (sp√§ter)
```json
{
  "@fal-ai/serverless-client": "^0.x.x",
  "openai": "^4.x.x",
  "sharp": "^0.33.x" // bereits vorhanden
}
```

### R2 Bucket Configuration
- Alle Prefixes im selben Bucket: `repl-default-bucket-{REPL_ID}`
- CORS-Policy f√ºr Self-Edit Canvas-Uploads erweitern
- Lifecycle-Rules f√ºr archive/ (Retention: 90 Tage)

### Database Migration Strategy
- Drizzle-Schema erweitern (images table)
- `npm run db:push` f√ºr neue Felder
- Backfill-Script f√ºr existing images (alle Flags auf false setzen)

---

## 10. Security & Performance

### Rate Limiting (fal.ai / OpenAI)
- Max concurrent Vision-Jobs: 5 (VISION_MAX_CONCURRENT_JOBS)
- Request Timeout: 30s (VISION_REQUEST_TIMEOUT_MS)
- Retry Count: 3 (VISION_JOB_RETRY_COUNT)
- GPT Rate Limit: 60 requests/min (OpenAI Standard)

### Object Storage Costs
- archive/: Highres-JPEGs (~5-10 MB/Bild)
- master/: 3000px JPEGs (~2-3 MB/Bild)
- analysis/: JSON + Masken (~500 KB/Bild)
- edits/: Self-Edit Versionen (~2 MB/Version)

**Sch√§tzung pro Shoot (40 Bilder)**:
- archive/: 400 MB
- master/: 120 MB
- analysis/: 20 MB
- **Total**: ~540 MB/Shoot

### KI-API Kosten (Sch√§tzung)
- fal.ai Florence-2: ~$0.01/Bild
- fal.ai SAM2: ~$0.02/Bild
- OpenAI GPT-Vision: ~$0.05/Bild
- **Total KI-Kosten**: ~$0.08/Bild ‚Üí ~$3.20/Shoot (40 Bilder)

---

## 11. Monitoring & Observability (zuk√ºnftig)

### Geplante Metriken
- Vision-Job Success Rate (%)
- GPT-Job Success Rate (%)
- Avg. Processing Time (Vision + GPT)
- R2 Storage Growth (GB/month)
- Self-Edit Usage (Objektentfernungen/Woche)
- Summary-Generation Errors

### Audit-Logs erweitern
```typescript
// Neue auditActionType values (FUTURE)
"vision_job_started"
"vision_job_completed"
"vision_job_failed"
"gpt_job_started"
"gpt_job_completed"
"gpt_job_failed"
"self_edit_object_removed"
"summary_generated"
"avatar_generated"
"video_generated"
```

---

## 12. Zusammenfassung

### Was IST dokumentiert
‚úÖ R2-Prefixes f√ºr alle Bildversionen  
‚úÖ DB-Felder f√ºr KI-Pipeline Status  
‚úÖ Editor-Return als KI-Einstiegspunkt  
‚úÖ Service-Orchestrator Struktur  
‚úÖ ENV-Variablen Naming-Convention  
‚úÖ Self-Edit API Workflow  
‚úÖ summary.json Schema  

### Was NICHT implementiert ist
‚è∏Ô∏è Keine neuen DB-Felder hinzugef√ºgt  
‚è∏Ô∏è Keine neuen API-Routes erstellt  
‚è∏Ô∏è Keine Service-Orchestratoren gebaut  
‚è∏Ô∏è Keine ENV-Variablen gesetzt  
‚è∏Ô∏è Keine fal.ai/OpenAI Integration  

### N√§chste Schritte (NACH HALT-Freigabe)
1. User-Freigabe f√ºr Phase 1 einholen
2. DB-Migration planen (images table extensions)
3. Editor-Return API implementieren
4. fal.ai Account + API-Keys einrichten
5. Vision-Orchestrator Prototyp bauen

---

**Dokumentation g√ºltig ab**: 2025-01-14  
**Letzte Aktualisierung**: 2025-01-14  
**Status**: üìã PLANNING (keine aktive Entwicklung)
